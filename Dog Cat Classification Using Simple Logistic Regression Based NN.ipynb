{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing a Basic Simple Program to Classify Dogs and Cats into separate classes using a basic Logistic Regression Based Neural network.\n",
    "\n",
    "Using Most Basic Libraries and will write custom functions for the NN.\n",
    "\n",
    "Libraries Used:\n",
    "\n",
    "- Numpy : For computational operations. \n",
    "- Pandas : DataFrame Based Manipulations \n",
    "- cv2 : For reading the image data. Used to read/ convert image into array and generate grayscale.\n",
    "- matplotlib : For plotting the images data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd;\n",
    "import numpy as np;\n",
    "import cv2;\n",
    "import matplotlib.pyplot as plt;\n",
    "import matplotlib;\n",
    "import os;\n",
    "from collections import Counter;\n",
    "import random; \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dogs-vs-cats.zip',\n",
       " 'Experiments-with-Deep-Learning-',\n",
       " 'sampleSubmission.csv',\n",
       " 'test1',\n",
       " 'test1.zip',\n",
       " 'train',\n",
       " 'train.zip']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('C:/Users/adity/Documents/Projects/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the data \n",
    "# os.listdir(\"C:\\\\Users\\\\adity\\\\Documents\\\\Projects\")\n",
    "\n",
    "maindir=\"C:/Users/adity/Documents/Projects/\"\n",
    "trainDir=\"train\"\n",
    "path=os.path.join(maindir,trainDir);\n",
    "X=[];\n",
    "y=[];\n",
    "for p in os.listdir(path):\n",
    "    class_val=p.split(\".\")[0];\n",
    "    img_array=cv2.imread(os.path.join(path,p),cv2.IMREAD_GRAYSCALE)\n",
    "    new_img_array=cv2.resize(img_array,dsize=(80,80));\n",
    "#     plt.imshow(new_img_array)\n",
    "    X.append(new_img_array);\n",
    "    y.append(class_val);\n",
    "X=np.array(X).reshape(-1,80,80,1);\n",
    "X_flatten=X.reshape(X.shape[0],-1).T\n",
    "y_flatten=np.array(y)\n",
    "\n",
    "# testDir=\"test1\"\n",
    "# path=os.path.join(maindir,testDir);\n",
    "# X_test=[];\n",
    "# y_test=[];\n",
    "# #Reading the Test Data.\n",
    "# for p in os.listdir(path):\n",
    "#     class_val=p.split(\".\");\n",
    "#     img_array=cv2.imread(os.path.join(path,p),cv.IMREAD_GRAYSCALE)\n",
    "#     new_img_array=cv.resize(img_array,dsize=(80,80))\n",
    "#     X_test.append(new_img_array);\n",
    "#     y_test.append(class_val);\n",
    "# X_test=np.array(X_test).reshape(-1,80,80,1)\n",
    "# X_test_flatten=X_test.reshape(X_test.shape[0],-1).T\n",
    "# y_test=np.array(y_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X in Training DataSet  (6400, 25000)\n",
      "Shape of y in Training Dataset (25000, 1)\n"
     ]
    }
   ],
   "source": [
    "y_train=y_flatten.reshape(y_flatten.shape[0],1)\n",
    "X_train=X_flatten.copy()\n",
    "\n",
    "print('Shape of X in Training DataSet ',X_train.shape)\n",
    "print('Shape of y in Training Dataset',y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardizing the Dataset \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.unique(y_train,return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Image Data Preprocessing.\n",
    "\n",
    "* Once we've read the data we'd want to preprocess and generate training and validation sets before proceeding to the test set and final results generation\n",
    "* Also, class cat will be assigned 1 and dog 0. ( Although I love dogs over cats ). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the data into training and validation portions \n",
    "X_train=X_train/255.\n",
    "data=pd.DataFrame(X_train).T\n",
    "data.shape\n",
    "data['Class']=y_train\n",
    "data_cat=data[data['Class']=='cat'].reset_index()\n",
    "data_dog=data[data['Class']=='dog'].reset_index()\n",
    "indices=random.sample(range(12500),12500)\n",
    "data_cat_train=data_cat.iloc[indices[:11250],:]\n",
    "data_cat_validation=data_cat.iloc[indices[11250:],:]\n",
    "data_dog_train=data_dog.iloc[indices[:11250],:]\n",
    "data_dog_validation=data_dog.iloc[indices[11250:],:]\n",
    "\n",
    "# data_cat_train.shape\n",
    "data_train=pd.concat([data_cat_train,data_dog_train])\n",
    "data_validation=pd.concat([data_cat_validation,data_dog_validation])\n",
    "\n",
    "data_train=data_train.reset_index().iloc[:,1:]\n",
    "data_train['NUM_CLASS']=0\n",
    "data_train.loc[data_train['Class']=='cat','NUM_CLASS']=1\n",
    "data_train=data_train.drop(['Class','index'],axis=1)\n",
    "data_train=data_train.T\n",
    "data_validation=data_validation.reset_index().iloc[:,1:]\n",
    "data_validation['NUM_CLASS']=0\n",
    "data_validation.loc[data_validation['Class']=='cat','NUM_CLASS']=1\n",
    "data_validation=data_validation.drop(['Class','index'],axis=1)\n",
    "data_validation=data_validation.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Train Shape (6401, 22500)\n",
      "Data Validation Shape (6401, 2500)\n"
     ]
    }
   ],
   "source": [
    "print('Data Train Shape',data_train.shape)\n",
    "print('Data Validation Shape',data_validation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Generating sigmoid of a value '''\n",
    "def sigmoid(z):\n",
    "    s=1/(1+np.exp(-z))\n",
    "    return s;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Parameter Initialization'''\n",
    "def initialize_parameters(dim):\n",
    "    w= np.zeros((dim,1));\n",
    "    b=np.zeros(1);\n",
    "    return w,b;\n",
    "\n",
    "# t_w,t_b=initialize_parameters(2);\n",
    "# print('w',t_w)\n",
    "# print('b',t_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Generating forward and backward propagation'''\n",
    "def propagate(w,b,X,Y):\n",
    "    m=X.shape[1]\n",
    "    #Forward Propagation\n",
    "    A=sigmoid(np.dot(w.T,X)+b); #Computing activation.\n",
    "    cost=(-1/m)*np.sum(np.multiply(Y,np.log(A))+ np.multiply((1-Y),np.log(1-A)))\n",
    "    #Backward Propagation \n",
    "    dw= (1/m)*np.dot(X,(A-Y).T)\n",
    "    db=1/m*(np.sum(A-Y))\n",
    "    assert(dw.shape == w.shape)\n",
    "    assert(db.dtype == float)\n",
    "    cost = np.squeeze(cost)\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return grads, cost\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]])\n",
    "# grads, cost = propagate(w, b, X, Y)\n",
    "# print (\"dw = \" + str(grads[\"dw\"]))\n",
    "# print (\"db = \" + str(grads[\"db\"]))\n",
    "# print (\"cost = \" + str(cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Iteration and Optimization of the parameters'''\n",
    "def optimize(w,b,X,Y,num_iterations,learning_rate,print_cost=False):\n",
    "    costs=[];\n",
    "    for i in range(num_iterations):\n",
    "        grads,cost=propagate(w,b,X,Y); #calculating cost and gradients.\n",
    "        dw = grads[\"dw\"]\n",
    "        db = grads[\"db\"]\n",
    "\n",
    "        w= w-( learning_rate*dw); #updating value of w.\n",
    "        b=b -( learning_rate*db); # updating value of b\n",
    "        \n",
    "        if i%100==0:\n",
    "            costs.append(cost);\n",
    "        \n",
    "        if print_cost and i%100==0:\n",
    "            print('Iteration number %i: %f :'%(i,cost));\n",
    "        \n",
    "        params={'w': w,'b':b};\n",
    "        grads={'dw':dw,'db':db}\n",
    "        \n",
    "    return params,grads,costs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Testing if the function actually works.\n",
    "# params, grads, costs = optimize(w, b, X, Y, num_iterations= 100, learning_rate = 0.009, print_cost = False)\n",
    "# print (\"w = \" + str(params[\"w\"]))\n",
    "# print (\"b = \" + str(params[\"b\"]))\n",
    "# print (\"dw = \" + str(grads[\"dw\"]))\n",
    "# print (\"db = \" + str(grads[\"db\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w,b,X):\n",
    "    m=X.shape[1]; # Getting  number of examples since the shape of X is (number_of_features, number of examples)\n",
    "    Y_prediction=np.zeros((1,m));\n",
    "    w= w.reshape(X.shape[0],1);\n",
    "    A=sigmoid(np.dot(w.T,X)+b);\n",
    "    \n",
    "    for i in range(A.shape[1]):\n",
    "        Y_prediction[:,i]=np.where(A[:,i]>=0.5,1,0);\n",
    "    assert (Y_prediction.shape==(1,m) )\n",
    "    return Y_prediction;\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w = np.array([[0.1124579],[0.23106775]])\n",
    "# b = -0.3\n",
    "# X = np.array([[1.,-1.1,-3.2],[1.2,2.,0.1]])\n",
    "# print (\"predictions = \" + str(predict(w, b, X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train,Y_train,X_test,Y_test,num_iterations=2000,learning_rate=0.5,print_cost=False):\n",
    "    \n",
    "    #Initialize parameters\n",
    "    w,b=initialize_parameters(X_train.shape[0]);\n",
    "#     print('X_train shape',X_train.shape,Y_train.shape)\n",
    "    \n",
    "    #Gradient Descent \n",
    "    params,grads,costs=optimize(w,b,X_train,Y_train,num_iterations,learning_rate,print_cost);\n",
    "    \n",
    "    #Getting the parameters from the trained model.\n",
    "    w=params['w']\n",
    "    b=params['b']\n",
    "    \n",
    "    #Generating predictions for training and testing examples \n",
    "    Y_prediction_train=predict(w,b,X_train)\n",
    "    Y_prediction_test=predict(w,b,X_test)\n",
    "    \n",
    "    #Print train/test Errors \n",
    "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
    "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
    "    \n",
    "    \n",
    "    d={'costs':costs,'Y_prediction_train':Y_prediction_train,'Y_prediction_test':Y_prediction_test,'w':w,'b':b,'learning_rate':learning_rate,'num_iterations':num_iterations}\n",
    "    return d;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_x=data_train.iloc[:-1,:].to_numpy()\n",
    "train_set_y=data_train.iloc[-1,:].to_numpy()\n",
    "test_set_x=data_validation.iloc[:-1,:].to_numpy()\n",
    "test_set_y=data_validation.iloc[-1,:].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 0: 0.693147 :\n",
      "Iteration number 100: 0.681091 :\n",
      "Iteration number 200: 0.678618 :\n",
      "Iteration number 300: 0.677396 :\n",
      "Iteration number 400: 0.676582 :\n",
      "Iteration number 500: 0.675947 :\n",
      "Iteration number 600: 0.675406 :\n",
      "Iteration number 700: 0.674921 :\n",
      "Iteration number 800: 0.674474 :\n",
      "Iteration number 900: 0.674057 :\n",
      "Iteration number 1000: 0.673661 :\n",
      "Iteration number 1100: 0.673283 :\n",
      "Iteration number 1200: 0.672920 :\n",
      "Iteration number 1300: 0.672570 :\n",
      "Iteration number 1400: 0.672232 :\n",
      "Iteration number 1500: 0.671903 :\n",
      "Iteration number 1600: 0.671583 :\n",
      "Iteration number 1700: 0.671271 :\n",
      "Iteration number 1800: 0.670966 :\n",
      "Iteration number 1900: 0.670667 :\n",
      "train accuracy: 59.56 %\n",
      "test accuracy: 57.72 %\n"
     ]
    }
   ],
   "source": [
    "d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 2000, learning_rate = 0.005, print_cost = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
