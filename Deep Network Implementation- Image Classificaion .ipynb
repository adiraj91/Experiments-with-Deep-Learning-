{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd;\n",
    "import numpy as np;\n",
    "import cv2;\n",
    "import matplotlib.pyplot as plt;\n",
    "import matplotlib;\n",
    "import os;\n",
    "from collections import Counter;\n",
    "import random; \n",
    "from importnb import Notebook;\n",
    "import time;\n",
    "import os;\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd();\n",
    "with Notebook():\n",
    "    import Data_Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X   (19200, 25000)\n",
      "Shape of y  (25000, 1)\n"
     ]
    }
   ],
   "source": [
    "elements=Data_Imports.readData(\"C:/Users/adity/Documents/Projects/\",\"train\")\n",
    "output_elements=Data_Imports.DataManipulation(elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def InitializeParametersDeep(layer_dimensions):\n",
    "    np.random.seed(3);\n",
    "    parameters={};\n",
    "    L=len(layer_dimensions);\n",
    "    \n",
    "    for i in range(1,L):\n",
    "        parameters['W'+str(i)]=np.random.randn(layer_dimensions[i],layer_dimensions[i-1])*0.01;\n",
    "        parameters['b'+str(i)]=np.zeros((layer_dimensions[i],1));\n",
    "        \n",
    "        assert(parameters['W'+str(i)].shape==(layer_dimensions[i],layer_dimensions[i-1]));\n",
    "        assert(parameters['b'+str(i)].shape==(layer_dimensions[i],1));\n",
    "    return parameters;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 (5, 4)\n",
      "b1 (5, 1)\n",
      "W2 (5, 5)\n",
      "b2 (5, 1)\n"
     ]
    }
   ],
   "source": [
    "test_params=InitializeParametersDeep([4,5,5])\n",
    "print('W1',test_params['W1'].shape);\n",
    "print('b1',test_params['b1'].shape);\n",
    "print('W2',test_params['W2'].shape);\n",
    "print('b2',test_params['b2'].shape);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sigmoid(Z):\n",
    "    cache={};\n",
    "    A=i/(1+np.exp(-Z));\n",
    "    cache['Z']=Z;\n",
    "    return A,cache;\n",
    "def ReLU(Z):\n",
    "    cache={};\n",
    "    A=max(0,Z);\n",
    "    cache['Z']=Z;\n",
    "    return A,cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A,W,b):\n",
    "    Z=np.dot(W,A)+b;\n",
    "    assert (Z.shape==(W.shape[0],A.shape[1]))\n",
    "    cache=(A,w,b);\n",
    "    return Z,cache;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev,W,b,activation):\n",
    "    ''' Input to the function is the previous layer value which is input to the new year , Weights and activation function type.'''\n",
    "    if activation=='Sigmoid':\n",
    "        Z,linear_cache=linear_forward(A_prev,W,b);\n",
    "        A,activation_cache=Sigmoid(Z);\n",
    "        assert A.shape==(W.shape[0],A_prev.shape[1]);\n",
    "        cache=(linear_cache,activation_cache);\n",
    "        return A,cache;\n",
    "    elif activation=='ReLU':\n",
    "        Z,linear_cache=linear_forward(A_prev,W,b);\n",
    "        A,activation_cache=ReLU(Z);\n",
    "        assert A.shape==(W.shape[0],A_prev.shape[1]);\n",
    "        cache=(linear_cache,activation_cache);\n",
    "        return A,cache;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_Layer_Forward(X,parameters):\n",
    "    caches=[]; # Generates cache to store linear state forward.\n",
    "    A=X; #Input Data\n",
    "    L=len(parameters); #Gives number of layers.\n",
    "    \n",
    "    for i in range(1,L):\n",
    "        A_prev=A;\n",
    "        A,cache=linear_activation_forward(A_prev,parameters['W'+str(i)],parameters['b'+str(i)],'ReLU');\n",
    "        caches.append(cache);\n",
    "    AL,cache= linear_activation_forward(A,parameters['W'+str(L)],parameters['b'+str(L)],'Sigmoid');\n",
    "    caches.append(cache);\n",
    "    assert(AL.shape==(1,X.shape[1]));\n",
    "    return AL,caches;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL,y):\n",
    "    m=Y.shape[1];\n",
    "    cost=(-1/m)* np.sum([np.multiply(Y,np.log(AL))[0],np.multiply((1-Y),np.log(1-AL))[0]])\n",
    "    cost=np.squeeze(cos);\n",
    "    assert(cost.shape==());\n",
    "    return cost;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ,cache):\n",
    "    A_prev,W,b=cache\n",
    "    m=A_perv.shape[1]\n",
    "    dW=(1/m)*np.dot(dZ,A_prev.T)\n",
    "    db=(1/m)*np.sum(dZ,keepdims=True,axis=1);\n",
    "    dA_prev=np.dot(dW.T,dZ);\n",
    "    assert(dA_prev.shape==A_prev.shape)\n",
    "    assert(dW.shape==W.shape)\n",
    "    assert(db.shape==b.shape)\n",
    "    return dA_prev,dW,db;\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_backward(dA,cache):\n",
    "    Z=cache;\n",
    "    dZ=np.array(da,copy=True);\n",
    "    dZ[Z<=0]=0;\n",
    "    assert(dZ.shape==Z.shape)\n",
    "    return dZ;\n",
    "\n",
    "def sigmoid_backward(dA,cache):\n",
    "    Z=cache;\n",
    "    s=1/(1+np.exp(-Z));\n",
    "    dZ=dA*s*(1-s);\n",
    "    assert(dZ.shape==Z.shape)\n",
    "    return dZ;\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward_activation(dA,cache,activation):\n",
    "    linear_cache, activation_cache=cache;\n",
    "    if activation=='sigmoid':\n",
    "        dZ=sigmoid_backward(dA,activation_cache);\n",
    "        dA_prev,dW,db=linear_backward(dZ,linear_cache);\n",
    "    elif activation=='relu':\n",
    "        dZ=relu_backward(dA,activation_cache);\n",
    "        dA_prev,dW,db=linear_backward(dZ,linear_cache);\n",
    "    return dA_prev,dW,db;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_backward(AL,Y,caches):\n",
    "    grads={};\n",
    "    m=AL.shape;\n",
    "    Y=Y.reshape(AL.shape);\n",
    "    dAL=np.divide(Y,AL)+np.divide((1-Y),(1-AL));\n",
    "    current_cache= caches[L-1];\n",
    "    grads['dA'+str(L-1)],grads['dW'+str(L)], grads['db'+str(L)]=linear_backward_activation(dAL,current_cache,'sigmoid');\n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        current_cache=caches[l];\n",
    "        grads['dA'+str(l)],grads['dW'+str(l+1)],grads['db'+str(l+1)]=linear_backward_activation(grads['dA'+str(l+1)],current_cache,'relu');\n",
    "    return grads;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters,grads,learning_rate):\n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    # Update rule for each parameter. Use a for loop.\n",
    "    ### START CODE HERE ### (â‰ˆ 3 lines of code)\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters['W'+str(l+1)]- (learning_rate*grads['dW'+str(l+1)]);\n",
    "        parameters[\"b\" + str(l+1)] = parameters['b'+str(l+1)]-(learning_rate*grads['db'+str(l+1)]);\n",
    "    ### END CODE HERE ###\n",
    "    return parameters\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
